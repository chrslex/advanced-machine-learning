{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def sigmoid(x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "        \n",
    "    def sigmoid_derivative(x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def relu(x):\n",
    "        return np.maximum(0,x) \n",
    "    \n",
    "    def relu_derivative(x):\n",
    "        return 1. * (x > 0)\n",
    "\n",
    "    def tanh(x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(x):\n",
    "        return 1 - x**2\n",
    "\n",
    "    def softmax(x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum()\n",
    "\n",
    "    def softmax_derivative(x):\n",
    "        return Layer.softmax(x) * (1 - Layer.softmax(x))\n",
    "\n",
    "    def cross_entropy(y_true, y_pred):\n",
    "        return -np.sum(y_true * np.log(y_pred))\n",
    "\n",
    "    def cross_entropy_derivative(y_true, y_pred):\n",
    "        return -y_true/y_pred\n",
    "\n",
    "    def mse(y_true, y_pred):\n",
    "        return np.mean(np.square(y_true - y_pred))\n",
    "\n",
    "    def mse_derivative(y_true, y_pred):\n",
    "        return 2 * (y_pred - y_true) / y_true.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, input_dim, output_dim, activation):\n",
    "        self.weights = np.random.randn(input_dim, output_dim) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        self.activation = activation\n",
    "        self.activation_output = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        self.activation_output = self.activation(np.dot(input, self.weights) + self.biases)\n",
    "        return self.activation_output\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == Layer.sigmoid:\n",
    "            return Layer.sigmoid_derivative(x)\n",
    "        elif self.activation == Layer.relu:\n",
    "            return Layer.relu_derivative(x)\n",
    "        elif self.activation == Layer.tanh:\n",
    "            return Layer.tanh_derivative(x)\n",
    "        elif self.activation == Layer.softmax:\n",
    "            return Layer.softmax_derivative(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "    \n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "   \n",
    "    def __repr__(self):\n",
    "        return \"Dense Layer\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"Dense Layer\"\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        return self.forward_propagation(input)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.weights.shape[1]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.weights[:, index], self.biases[:, index]\n",
    "    \n",
    "    def __setitem__(self, index, value):\n",
    "        self.weights[:, index] = value[0]\n",
    "        self.biases[:, index] = value[1]\n",
    "    \n",
    "    def __delitem__(self, index):\n",
    "        self.weights = np.delete(self.weights, index, axis=1)\n",
    "        self.biases = np.delete(self.biases, index, axis=1)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(self.weights.shape[1]):\n",
    "            yield self.weights[:, i], self.biases[:, i]\n",
    "    \n",
    "    def __reversed__(self):\n",
    "        for i in reversed(range(self.weights.shape[1])):\n",
    "            yield self.weights[:, i], self.biases[:, i]\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.weights or item in self.biases\n",
    "    \n",
    "    def update(self, weights, biases):\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "    \n",
    "    def save(self, path):\n",
    "        np.savez(path, weights=self.weights, biases=self.biases)\n",
    "    \n",
    "    def load(self, path):\n",
    "        data = np.load(path)\n",
    "        self.weights = data['weights']\n",
    "        self.biases = data['biases']\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "     \n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "    \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "        self.biases = np.zeros((1, weights.shape[1]))\n",
    "\n",
    "    def set_biases(self, biases):\n",
    "        self.biases = biases\n",
    "        self.weights = np.zeros((biases.shape[1], 1))\n",
    "\n",
    "    def set_activation(self, activation):\n",
    "        self.activation = activation\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convo2D:\n",
    "    def __init__(self, input_dim, output_dim, activation, filter_size, stride, padding):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.filters = np.random.randn(output_dim, input_dim, filter_size, filter_size) * np.sqrt(2.0 / (input_dim + output_dim))\n",
    "        self.biases = np.zeros((1, output_dim))\n",
    "        self.activation_output = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        self.output_dim = int((self.input_dim - self.filter_size + 2 * self.padding) / self.stride + 1)\n",
    "        self.activation_output = np.zeros((self.output_dim, self.output_dim, self.output_dim))\n",
    "        for f in range(self.output_dim):\n",
    "            for i in range(self.output_dim):\n",
    "                for j in range(self.output_dim):\n",
    "                    self.activation_output[f, i, j] = np.sum(self.input[:, i*self.stride:i*self.stride+self.filter_size, j*self.stride:j*self.stride+self.filter_size] * self.filters[f]) + self.biases[f]\n",
    "        self.activation_output = self.activation(self.activation_output)\n",
    "        return self.activation_output\n",
    "\n",
    "    def activation_derivative(self, x):\n",
    "        if self.activation == Layer.sigmoid:\n",
    "            return Layer.sigmoid_derivative(x)\n",
    "        elif self.activation == Layer.relu:\n",
    "            return Layer.relu_derivative(x)\n",
    "        elif self.activation == Layer.tanh:\n",
    "            return Layer.tanh_derivative(x)\n",
    "        elif self.activation == Layer.softmax:\n",
    "            return Layer.softmax_derivative(x)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "\n",
    "    def get_biases(self):\n",
    "        return self.biases\n",
    "    \n",
    "    def update(self, filters, biases):\n",
    "        self.filters = filters\n",
    "        self.biases = biases\n",
    "    \n",
    "    def save(self, path):\n",
    "        np.savez(path, filters=self.filters, biases=self.biases)\n",
    "    \n",
    "    def load(self, path):\n",
    "        data = np.load(path)\n",
    "        self.filters = data['filters']\n",
    "        self.biases = data['biases']\n",
    "\n",
    "    def updateShape(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = int((self.input_dim - self.filter_size + 2 * self.padding) / self.stride + 1)\n",
    "        self.activation_output = np.zeros((self.output_dim, self.output_dim, self.output_dim))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def input(self, input):\n",
    "        self.layers[0].input = input\n",
    "        self.layers[0].output = input\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i].input = self.layers[i-1].output\n",
    "            self.layers[i].output = self.layers[i].forward_propagation(self.layers[i].input)\n",
    "        return self.layers[-1].output\n",
    "    \n",
    "    def output(self, output):\n",
    "        self.layers[-1].output = output\n",
    "        self.layers[-1].input = output\n",
    "        for i in reversed(range(len(self.layers)-1)):\n",
    "            self.layers[i].output = self.layers[i+1].input\n",
    "            self.layers[i].input = self.layers[i].backward_propagation(self.layers[i].output)\n",
    "        return self.layers[0].input\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        activations = []\n",
    "        input = input\n",
    "        for layer in self.layers:\n",
    "            activations.append(layer.forward_propagation(input))\n",
    "            input = activations[-1]\n",
    "        return activations\n",
    "\n",
    "    def train(self, X, y, learning_rate, epochs):\n",
    "        mses = []\n",
    "        for i in range(epochs):\n",
    "            sum_error = 0\n",
    "            for j in range(len(X)):\n",
    "                activation = self.forward_propagation(X[j])\n",
    "                error = y[j] - activation[-1]\n",
    "                self.backward_propagation(error, learning_rate)\n",
    "                sum_error += np.sum(error)\n",
    "            print('>epoch=%d, lrate=%.3f, error=%.3f' % (i, learning_rate, sum_error))\n",
    "            mses.append(sum_error)\n",
    "        return mses\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = []\n",
    "        for i in range(len(X)):\n",
    "            activation = self.forward_propagation(X[i])\n",
    "            y_pred.append(activation[-1])\n",
    "    \n",
    "            red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_neural_network():\n",
    "    model = Sequential()\n",
    "    model.add(Convo2D(1, 32, Layer.relu, 3, 1, 1))\n",
    "    model.add(Convo2D(32, 64, Layer.relu, 3, 1, 1))\n",
    "    model.add(Dense(64, 10, Layer.softmax))\n",
    "    return model\n",
    "\n",
    "def convo2d_test():\n",
    "    model = convolutional_neural_network()\n",
    "    X = np.random.randn(1, 28, 28)\n",
    "    y = np.random.randn(1, 10)\n",
    "    model.train(X, y, 0.1, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling2D ():\n",
    "    def __init__(self, input_dim, output_dim, activation, filter_size, stride, padding):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.activation_output = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        self.output_dim = int((self.input_dim - self.filter_size + 2 * self.padding) / self.stride + 1)\n",
    "        self.activation_output = np.zeros((self.output_dim, self.output_dim, self.output_dim))\n",
    "        for f in range(self.output_dim):\n",
    "            for i in range(self.output_dim):\n",
    "                for j in range(self.output_dim):\n",
    "                    self.activation_output[f, i, j] = np.max(self.input[:, i*self.stride:i*self.stride+self.filter_size, j*self.stride:j*self.stride+self.filter_size])\n",
    "        self.activation_output = self.activation(self.activation_output)\n",
    "        return self.activation_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten ():\n",
    "    def __init__(self, input_dim, output_dim, activation):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.activation = activation\n",
    "        self.activation_output = None\n",
    "        self.error = None\n",
    "        self.delta = None\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        self.input = input\n",
    "        self.activation_output = self.input.flatten()\n",
    "        self.activation_output = self.activation(self.activation_output)\n",
    "        return self.activation_output\n",
    "\n",
    "    def save(self, path):\n",
    "        np.savez(path, filters=self.filters, biases=self.biases)\n",
    "    \n",
    "    def load(self, path):\n",
    "        data = np.load(path)\n",
    "        self.filters = data['filters']\n",
    "        self.biases = data['biases']\n",
    "\n",
    "    def updateShape(self, input_dim):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = int((self.input_dim - self.filter_size + 2 * self.padding) / self.stride + 1)\n",
    "        self.activation_output = np.zeros((self.output_dim, self.output_dim, self.output_dim))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2806249540.py, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [221], line 43\u001b[1;36m\u001b[0m\n\u001b[1;33m    t the predictions\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import as_float_array\n",
    "\n",
    "np.random.seed(1000)\n",
    "image_size = (100,100)\n",
    "\n",
    "def read_image_array(path):\n",
    "    path = glob.glob(path)\n",
    "    images = []\n",
    "\n",
    "    for image in path:\n",
    "        img = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, image_size)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "cats = read_image_array('./cats/*.jpg')\n",
    "dogs = read_image_array('./dogs/*.jpg')\n",
    "\n",
    "# Make an array of 0s for cats and 1s for dogs\n",
    "X = np.array(cats + dogs)\n",
    "y = np.array([0] * len(cats) + [1] * len(dogs))\n",
    "\n",
    "# Normalize the data\n",
    "X = X / 255.0 # 255 is the max value of a pixel\n",
    "X = X.reshape(-1, 100, 100, 1) # -1 is for all images, 100x100 is the size of the image, 1 is for grayscale\n",
    "y = y.reshape(-1, 1) # -1 is for all images, 1 is for grayscale\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "Sequential = Sequential() # Create the model\n",
    "Sequential.add(Convo2D(1, 32, Layer.relu, 3, 1, 1)) # Add a convolutional layer with 32 filters, a 3x3 filter size and a relu activation function, with a stride of 1 and padding of 1 (so the output is the same size as the input)\n",
    "Sequential.add(Convo2D(32, 64, Layer.relu, 3, 1, 1)) # Add another convolutional layer with 64 filters, a 3x3 filter size and a relu activation function, with a stride of 1 and padding of 1 (so the output is the same size as the input)\n",
    "Sequential.add(Pooling2D(64, 64, Layer.relu, 2, 2, 0)) # Add a pooling layer with a 2x2 filter size and a stride of 2 \n",
    "Sequential.add(Flatten(64, 10, Layer.softmax)) # Add a flatten layer to turn the 2D array into a 1D array so we can use it in a dense layer with a softmax activation function to get the probabilities for each class\n",
    "Sequential.add(Dense(10, 1, Layer.sigmoid)) # Add a dense layer with 1 output (for the probability of the image being a dog) and a sigmoid activation function\n",
    "\n",
    "Sequential.train(X_train, y_train, 0.1, 10) # Train the model with a learning rate of 0.1 and 10 epochs\n",
    "pred = Sequential.predict(X_test)  # Predict the probabilities for each class\n",
    "print(pred)  # Print the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3d42ad487b2b2994e012ab0306cc0a65d83ea9d079e1ade1e8453053e613de7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
